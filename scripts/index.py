
import pandas as pd
import numpy as np
import os
import logging
import warnings
from sqlalchemy import create_engine, text, Table, Column, String, Date, MetaData, Float, Numeric
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.cluster import KMeans
import nltk
from textblob import TextBlob
from textblob_fr import PatternAnalyzer
from collections import Counter
import re

# D√©sactiver le GPU
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

class ReviewAnalysisConfig:
    DATABASE_URI = "postgresql://elhassan:elhassan@localhost:5432/google_reviews_db"
    LOG_FILE = "review_analysis.log"
    BATCH_SIZE = 100
    NUM_TOPICS = 15  # Augment√© pour plus de granularit√©
    NLTK_PATH = "/home/elhassan/nltk_data"

class ImprovedReviewAnalyzer:
    def __init__(self):
        self._setup_logging()
        self._configure_environment()
        self.engine = None
        self.nlp = None
        self.french_stopwords = None
        self._init_topic_mappings()

    def _init_topic_mappings(self):
        """Initialise les mappings pour une meilleure interpr√©tation des topics"""
        self.topic_keywords_mapping = {
            'service_quality_excellent': ['excellent', 'parfait', 'exceptionnel', 'qualit√©', 'top'],
            'service_quality_poor': ['mauvais', 'nul', 'catastrophique', 'pire', 'horrible'],
            'personnel_competent': ['personnel', 'professionnel', 'comp√©tent', 'serviable', 'aimable'],
            'personnel_problematic': ['impoli', 'incomp√©tent', 'd√©sagr√©able', 'agent', 'staff'],
            'waiting_time_long': ['attendre', 'attente', 'lent', 'temps', 'file', 'queue'],
            'waiting_time_quick': ['rapide', 'vite', 'efficace', 'imm√©diat', 'prompt'],
            'phone_communication': ['t√©l√©phone', 'appeler', 'communication', 'num√©ro', 'ligne'],
            'physical_infrastructure': ['agence', 'banque', 'b√¢timent', 'locaux', 'emplacement'],
            'atm_services': ['guichet', 'distributeur', 'automatique', 'retrait', 'd√©p√¥t'],
            'account_operations': ['compte', 'op√©ration', 'transaction', 'virement', 'solde'],
            'fees_charges': ['frais', 'commission', 'co√ªt', 'tarif', 'prix'],
            'opening_hours': ['horaire', 'ouverture', 'fermeture', 'heure', 'temps'],
            'security_issues': ['s√©curit√©', 'vol', 'fraude', 'probl√®me', 'risque'],
            'digital_services': ['application', 'site', 'internet', 'num√©rique', 'en_ligne'],
            'customer_experience': ['exp√©rience', 'satisfaction', 'recommander', '√©viter', 'client']
        }
        
        self.topic_names_fr = {
            'service_quality_excellent': 'Service Excellent',
            'service_quality_poor': 'Service D√©faillant',
            'personnel_competent': 'Personnel Comp√©tent',
            'personnel_problematic': 'Personnel Probl√©matique',
            'waiting_time_long': 'Temps d\'Attente Excessif',
            'waiting_time_quick': 'Service Rapide',
            'phone_communication': 'Communication T√©l√©phonique',
            'physical_infrastructure': 'Infrastructure Physique',
            'atm_services': 'Services Guichet Automatique',
            'account_operations': 'Op√©rations de Compte',
            'fees_charges': 'Frais et Tarification',
            'opening_hours': 'Horaires d\'Ouverture',
            'security_issues': 'Questions de S√©curit√©',
            'digital_services': 'Services Num√©riques',
            'customer_experience': 'Exp√©rience Client Globale'
        }

    def _setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(ReviewAnalysisConfig.LOG_FILE),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _configure_environment(self):
        warnings.filterwarnings('ignore')
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
        nltk.data.path.append(ReviewAnalysisConfig.NLTK_PATH)

    def setup_nlp_resources(self):
        try:
            self._download_nltk_resources()
            self._load_spacy_model()
            self.logger.info("Ressources NLP fran√ßaises charg√©es")
        except Exception as e:
            self.logger.error(f"Erreur NLP : {e}")
            raise

    def _download_nltk_resources(self):
        try:
            nltk.data.find('corpora/stopwords', paths=[ReviewAnalysisConfig.NLTK_PATH])
        except LookupError:
            nltk.download('stopwords', quiet=True, download_dir=ReviewAnalysisConfig.NLTK_PATH)
            
        from nltk.corpus import stopwords
        self.french_stopwords = set(stopwords.words('french'))
        
        # Ajouter des stopwords sp√©cifiques au domaine bancaire
        banking_stopwords = {'banque', 'agence', 'aller', 'faire', '√™tre', 'avoir', 'dire', 'voir', 'plus', 'bien', 'tout', 'fois', 'tr√®s', 'toujours', 'jamais'}
        self.french_stopwords.update(banking_stopwords)

    def _load_spacy_model(self):
        try:
            self.nlp = spacy.load("fr_core_news_sm")
        except OSError:
            os.system("python -m spacy download fr_core_news_sm")
            self.nlp = spacy.load("fr_core_news_sm")

    def analyze_sentiment(self, text):
        """Analyse le sentiment en fran√ßais avec une approche NLP hybride"""
        try:
            if not isinstance(text, str) or len(text.strip()) < 3:
                return 'neutre'
            
            # Nettoyer le texte
            text_clean = text.lower().strip()
            
            # Analyse avec TextBlob-FR (plus pr√©cise pour le fran√ßais)
            try:
                blob = TextBlob(text_clean, analyzer=PatternAnalyzer())
                polarity = blob.sentiment[0]
                
                # Analyse lexicale compl√©mentaire avec des mots-cl√©s fran√ßais
                positive_words = [
                    'excellent', 'parfait', 'g√©nial', 'super', 'formidable', 'magnifique',
                    'rapide', 'efficace', 'professionnel', 'serviable', 'aimable', 'courtois',
                    'satisfait', 'content', 'heureux', 'recommande', 'top', 'bien', 'bon',
                    'agr√©able', 'sympa', 'souriant', 'accueillant', 'comp√©tent'
                ]
                
                negative_words = [
                    'horrible', 'nul', 'catastrophique', 'd√©sastreux', 'inacceptable',
                    'lent', 'incomp√©tent', 'impoli', 'd√©sagr√©able', 'mauvais', 'pire',
                    'd√©√ßu', 'm√©content', 'frustr√©', '√©nerv√©', '√©viter', 'fuyez',
                    'scandaleux', 'inadmissible', 'honte', 'z√©ro', 'aucun', 'jamais'
                ]
                
                # Compter les mots positifs et n√©gatifs
                words = text_clean.split()
                pos_count = sum(1 for word in words if any(pw in word for pw in positive_words))
                neg_count = sum(1 for word in words if any(nw in word for nw in negative_words))
                
                # Combiner TextBlob et analyse lexicale
                lexical_score = (pos_count - neg_count) / max(len(words), 1)
                
                # Score final combin√© (70% TextBlob + 30% lexical)
                final_score = 0.7 * polarity + 0.3 * lexical_score
                
                # Classification avec seuils ajust√©s
                if final_score > 0.1:
                    return 'positif'
                elif final_score < -0.1:
                    return 'n√©gatif'
                else:
                    return 'neutre'
                    
            except Exception:
                # Fallback sur analyse lexicale seule
                words = text_clean.split()
                pos_count = sum(1 for word in words if any(pw in word for pw in positive_words))
                neg_count = sum(1 for word in words if any(nw in word for nw in negative_words))
                
                if pos_count > neg_count:
                    return 'positif'
                elif neg_count > pos_count:
                    return 'n√©gatif'
                else:
                    return 'neutre'
                    
        except Exception as e:
            self.logger.warning(f"Erreur analyse sentiment: {e}")
            return 'neutre'

    def connect_database(self):
        try:
            self.engine = create_engine(ReviewAnalysisConfig.DATABASE_URI)
            self.logger.info("Connexion DB r√©ussie")
            return self.engine
        except Exception as e:
            self.logger.error(f"Erreur connexion DB : {e}")
            raise

    def fetch_reviews(self):
        try:
            with self.engine.connect() as connection:
                query = """
                    SELECT 
                        bank_name,
                        branch_name,
                        city,
                        location,
                        review_text_cleaned,
                        rating,
                        review_date::date
                    FROM public_public.cleaned_reviews
                    WHERE review_text_cleaned IS NOT NULL
                      AND review_text_cleaned != ''
                      AND rating IS NOT NULL
                """
                result = connection.execute(text(query))
                df = pd.DataFrame(result.fetchall(), columns=result.keys())
                
                # Validation et nettoyage des donn√©es
                self.logger.info(f"Reviews brutes r√©cup√©r√©es : {len(df)}")
                
                # Convertir les types de donn√©es
                df['rating'] = pd.to_numeric(df['rating'], errors='coerce')
                df['review_date'] = pd.to_datetime(df['review_date'], errors='coerce')
                
                # Supprimer les lignes avec des donn√©es critiques manquantes
                initial_count = len(df)
                df = df.dropna(subset=['rating', 'review_text_cleaned'])
                
                # Log des statistiques de nettoyage
                cleaned_count = len(df)
                if initial_count != cleaned_count:
                    self.logger.info(f"Lignes supprim√©es lors du nettoyage : {initial_count - cleaned_count}")
                
                self.logger.info(f"Reviews finales apr√®s nettoyage : {cleaned_count}")
                return df
                
        except Exception as e:
            self.logger.error(f"Erreur r√©cup√©ration : {e}")
            raise

    def preprocess_text(self, text):
        if not isinstance(text, str) or len(text.strip()) < 10:
            return []
        
        try:
            # Nettoyer le texte
            text = re.sub(r'[^\w\s]', ' ', text.lower())
            text = re.sub(r'\s+', ' ', text).strip()
            
            doc = self.nlp(text[:1000])
            tokens = []
            
            for token in doc:
                if (not token.is_stop and 
                    not token.is_punct and 
                    not token.is_space and
                    len(token.lemma_) > 2 and
                    token.lemma_.lower() not in self.french_stopwords and
                    token.pos_ in ['NOUN', 'ADJ', 'VERB']):
                    tokens.append(token.lemma_.lower())
            
            return tokens
        except Exception as e:
            self.logger.warning(f"Erreur pr√©traitement : {e}")
            return []

    def _categorize_topic(self, keywords):
        """Cat√©gorise un topic bas√© sur ses mots-cl√©s"""
        keyword_scores = {}
        
        for category, cat_keywords in self.topic_keywords_mapping.items():
            score = sum(1 for kw in keywords if kw.lower() in [ck.lower() for ck in cat_keywords])
            if score > 0:
                keyword_scores[category] = score
        
        if keyword_scores:
            best_category = max(keyword_scores.items(), key=lambda x: x[1])[0]
            return self.topic_names_fr[best_category]
        
        return self._generate_contextual_name(keywords)

    def _generate_contextual_name(self, keywords):
        """G√©n√®re un nom contextuel bas√© sur l'analyse des mots-cl√©s"""
        # Analyser les sentiments implicites dans les mots-cl√©s
        positive_words = ['excellent', 'bon', 'meilleur', 'professionnel', 'rapide', 'serviable']
        negative_words = ['mauvais', 'pire', 'lent', 'nul', 'impoli', '√©viter']
        
        pos_count = sum(1 for kw in keywords if kw.lower() in positive_words)
        neg_count = sum(1 for kw in keywords if kw.lower() in negative_words)
        
        # Identifier les aspects principaux
        if any(kw in keywords for kw in ['service', 'qualit√©']):
            if neg_count > pos_count:
                return "Service Insatisfaisant"
            elif pos_count > neg_count:
                return "Service de Qualit√©"
            else:
                return "√âvaluation du Service"
        
        elif any(kw in keywords for kw in ['personnel', 'agent', 'accueil']):
            if neg_count > pos_count:
                return "Personnel Probl√©matique"
            elif pos_count > neg_count:
                return "Personnel Comp√©tent"
            else:
                return "√âvaluation du Personnel"
        
        elif any(kw in keywords for kw in ['attendre', 'temps', 'lent']):
            return "Gestion du Temps d'Attente"
        
        elif any(kw in keywords for kw in ['t√©l√©phone', 'communication']):
            return "Communication T√©l√©phonique"
        
        else:
            # Utiliser les 2 mots-cl√©s les plus significatifs
            main_keywords = [kw for kw in keywords[:3] if len(kw) > 3]
            if len(main_keywords) >= 2:
                return f"{main_keywords[0].capitalize()} & {main_keywords[1].capitalize()}"
            elif len(main_keywords) == 1:
                return main_keywords[0].capitalize()
            else:
                return "Aspects Divers"

    def extract_topics(self, processed_texts):
        try:
            # Filtrer les textes vides
            valid_texts = [text for text in processed_texts if text.strip()]
            
            if len(valid_texts) < 20:  # Augment√© le minimum pour plus de topics
                self.logger.warning("Pas assez de textes valides pour l'analyse th√©matique")
                return ["Donn√©es Insuffisantes"] * len(processed_texts), {}, [0.0] * len(processed_texts), ["Topic_0"] * len(processed_texts)

            vectorizer = TfidfVectorizer(
                stop_words=list(self.french_stopwords),
                max_features=3000,  # Augment√© pour plus de vocabulaire
                min_df=2,           # R√©duit pour capturer plus de nuances
                max_df=0.9,         # Augment√© l√©g√®rement
                ngram_range=(1, 3)  # Inclure les trigrammes pour plus de contexte
            )
            
            X = vectorizer.fit_transform(valid_texts)
            
            # Utiliser LDA avec des param√®tres optimis√©s pour plus de topics
            lda = LatentDirichletAllocation(
                n_components=ReviewAnalysisConfig.NUM_TOPICS,
                random_state=42,
                max_iter=100,       # Plus d'it√©rations pour la convergence
                learning_method='batch',
                learning_decay=0.7,
                doc_topic_prior=0.05,   # R√©duit pour plus de sp√©cificit√©
                topic_word_prior=0.005  # R√©duit pour des topics plus distincts
            )
            
            lda_output = lda.fit_transform(X)
            
            return self._process_improved_lda_results(lda, vectorizer, lda_output, processed_texts)
            
        except Exception as e:
            self.logger.error(f"Erreur topics : {e}")
            return ["Erreur d'Analyse"] * len(processed_texts), {}, [0.0] * len(processed_texts), ["Topic_Error"] * len(processed_texts)

    def _process_improved_lda_results(self, lda_model, vectorizer, lda_output, original_texts):
        feature_names = vectorizer.get_feature_names_out()
        themes = {}
        
        # G√©n√©rer des th√®mes am√©lior√©s
        for i, topic in enumerate(lda_model.components_):
            # Obtenir les mots-cl√©s les plus importants
            top_indices = topic.argsort()[:-20:-1]  # Top 20 mots
            top_keywords = [feature_names[idx] for idx in top_indices]
            
            # Filtrer les mots-cl√©s peu significatifs
            filtered_keywords = [kw for kw in top_keywords if len(kw) > 2 and not kw.isdigit()]
            
            # G√©n√©rer un nom de th√®me contextuel
            theme_name = self._categorize_topic(filtered_keywords[:10])
            
            themes[f"Topic_{i}"] = {
                'name': theme_name,
                'keywords': ', '.join(filtered_keywords[:10]),
                'weight': topic.sum()
            }

        # Attribution des topics aux reviews
        topic_names = []
        topic_labels = []
        topic_scores = []
        
        text_index = 0
        for original_text in original_texts:
            if original_text.strip():  # Texte valide
                if text_index < len(lda_output):
                    topic_distribution = lda_output[text_index]
                    dominant_topic = np.argmax(topic_distribution)
                    confidence = topic_distribution[dominant_topic]
                    
                    topic_labels.append(f"Topic_{dominant_topic}")
                    topic_names.append(themes[f"Topic_{dominant_topic}"]['name'])
                    topic_scores.append(confidence)
                    text_index += 1
                else:
                    topic_labels.append("Topic_0")
                    topic_names.append(themes["Topic_0"]['name'])
                    topic_scores.append(0.0)
            else:  # Texte vide
                topic_labels.append("Topic_0")
                topic_names.append("Donn√©es Insuffisantes")
                topic_scores.append(0.0)
        
        return topic_names, themes, topic_scores, topic_labels

    def create_enriched_table(self):
        metadata = MetaData()
        Table(
            'enriched_reviews', metadata,
            Column('bank_name', String),
            Column('branch_name', String),
            Column('city', String),  # Ajout de la colonne city
            Column('location', String),
            Column('review_text_cleaned', String),
            Column('rating', Numeric),
            Column('review_date', Date),
            Column('processed_text', String),
            Column('topic', String),
            Column('topic_confidence', Float),
            Column('sentiment', String),
            schema='public_public'
        )
        with self.engine.begin() as conn:
            if not conn.dialect.has_table(conn, 'enriched_reviews', schema='public_public'):
                metadata.create_all(conn)
                self.logger.info("Table enriched_reviews cr√©√©e avec la colonne city")

    def save_results(self, df):
        try:
            self.create_enriched_table()
            
            # Sauvegarder avec la colonne city incluse
            df_to_save = df[['bank_name', 'branch_name', 'city', 'location', 
                           'review_text_cleaned', 'rating', 'review_date',
                           'processed_text', 'topic', 'topic_confidence',
                           'sentiment']].copy()
            df_to_save['city']=df_to_save['city'].str.strip().str.replace('+', ' ').str.title()
            df_to_save.to_sql(
                name='enriched_reviews',
                con=self.engine,
                schema='public_public',
                if_exists='replace',
                index=False,
                chunksize=ReviewAnalysisConfig.BATCH_SIZE
            )
            self.logger.info(f"{len(df_to_save)} avis enrichis sauvegard√©s avec la colonne city")
        except Exception as e:
            self.logger.error(f"Erreur sauvegarde : {e}")
            raise

    def run_analysis(self):
        try:
            self.setup_nlp_resources()
            self.connect_database()
            
            df = self.fetch_reviews()
            
            # Pr√©traitement am√©lior√©
            self.logger.info("D√©but du pr√©traitement...")
            df['processed_text'] = df['review_text_cleaned'].apply(
                lambda x: ' '.join(self.preprocess_text(x)) if isinstance(x, str) else ""
            )
            
            # Analyse de sentiment
            self.logger.info("Analyse de sentiment en cours...")
            df['sentiment'] = df['review_text_cleaned'].apply(self.analyze_sentiment)
            
            # Analyse th√©matique am√©lior√©e
            self.logger.info("Analyse th√©matique am√©lior√©e en cours...")
            df['topic'], topics, df['topic_confidence'], topic_labels = self.extract_topics(
                df['processed_text'].tolist()
            )

            # Sauvegarde avec la colonne city incluse
            self.save_results(df[['bank_name', 'branch_name', 'city', 'location', 
                                'review_text_cleaned', 'rating', 'review_date',
                                'processed_text', 'topic', 'topic_confidence',
                                'sentiment']])
            
            return df, topics
        except Exception as e:
            self.logger.error(f"√âchec de l'analyse : {e}")
            raise

    def generate_summary_report(self, df, topics):
        """G√©n√®re un rapport de synth√®se des analyses"""
        print("\n" + "="*80)
        print("RAPPORT D'ANALYSE DES REVIEWS BANCAIRES")
        print("="*80)
        
        print(f"\nüìä STATISTIQUES G√âN√âRALES:")
        print(f"   ‚Ä¢ Nombre total de reviews analys√©es: {len(df)}")
        print(f"   ‚Ä¢ Note moyenne: {df['rating'].mean():.2f}/5")
        
        # Gestion s√©curis√©e des dates
        try:
            valid_dates = df['review_date'].dropna()
            if len(valid_dates) > 0:
                min_date = valid_dates.min()
                max_date = valid_dates.max()
                print(f"   ‚Ä¢ P√©riode: {min_date} √† {max_date}")
                print(f"   ‚Ä¢ Reviews avec dates valides: {len(valid_dates)}/{len(df)}")
            else:
                print(f"   ‚Ä¢ P√©riode: Dates non disponibles")
        except Exception as e:
            print(f"   ‚Ä¢ P√©riode: Erreur dans les dates ({str(e)})")
        
        print(f"\nüí≠ DISTRIBUTION DES SENTIMENTS:")
        sentiment_counts = df['sentiment'].value_counts()
        for sentiment, count in sentiment_counts.items():
            percentage = (count / len(df)) * 100
            print(f"   ‚Ä¢ {sentiment.capitalize()}: {count} ({percentage:.1f}%)")
        
        print(f"\nüéØ TH√àMES IDENTIFI√âS:")
        topic_counts = df['topic'].value_counts()
        for i, (topic, count) in enumerate(topic_counts.items(), 1):
            percentage = (count / len(df)) * 100
            print(f"   {i}. {topic}: {count} reviews ({percentage:.1f}%)")
        
        print(f"\nüîç D√âTAILS DES TH√àMES:")
        for topic_id, details in topics.items():
            print(f"\n   üìå {details['name']}:")
            print(f"      Mots-cl√©s: {details['keywords']}")
        
        print(f"\nüè¶ PERFORMANCE PAR BANQUE:")
        try:
            bank_performance = df.groupby('bank_name').agg({
                'rating': 'mean',
                'sentiment': lambda x: (x == 'positif').sum() / len(x) * 100
            }).round(2)
            
            for bank, stats in bank_performance.iterrows():
                print(f"   ‚Ä¢ {bank}: Note {stats['rating']}/5, {stats['sentiment']:.1f}% positif")
        except Exception as e:
            print(f"   ‚Ä¢ Erreur dans l'analyse par banque: {str(e)}")
        
        # Analyse par ville
        print(f"\nüèôÔ∏è PERFORMANCE PAR VILLE:")
        try:
            city_performance = df.groupby('city').agg({
                'rating': 'mean',
                'sentiment': lambda x: (x == 'positif').sum() / len(x) * 100
            }).round(2).head(10)  # Top 10 des villes
            
            for city, stats in city_performance.iterrows():
                print(f"   ‚Ä¢ {city}: Note {stats['rating']}/5, {stats['sentiment']:.1f}% positif")
        except Exception as e:
            print(f"   ‚Ä¢ Erreur dans l'analyse par ville: {str(e)}")
        
        # Ajouter des statistiques suppl√©mentaires
        print(f"\nüìà STATISTIQUES D√âTAILL√âES:")
        print(f"   ‚Ä¢ Reviews positives (rating ‚â• 4): {len(df[df['rating'] >= 4])}")
        print(f"   ‚Ä¢ Reviews n√©gatives (rating ‚â§ 2): {len(df[df['rating'] <= 2])}")
        print(f"   ‚Ä¢ Confiance moyenne des topics: {df['topic_confidence'].mean():.3f}")
        print(f"   ‚Ä¢ Nombre de villes diff√©rentes: {df['city'].nunique()}")
        
        # Top 3 des topics les plus fr√©quents
        top_topics = topic_counts.head(3)
        print(f"\nüîù TOP 3 DES PR√âOCCUPATIONS:")
        for i, (topic, count) in enumerate(top_topics.items(), 1):
            percentage = (count / len(df)) * 100
            print(f"   {i}. {topic} ({percentage:.1f}%)")

def main():
    analyzer = ImprovedReviewAnalyzer()
    try:
        results, topics = analyzer.run_analysis()
        
        # G√©n√©rer le rapport am√©lior√©
        analyzer.generate_summary_report(results, topics)
        
        print("\n‚úÖ Analyse termin√©e avec succ√®s!")
        print(f"üíæ R√©sultats sauvegard√©s dans la table 'enriched_reviews' avec la colonne city")
        
    except Exception as e:
        print(f"\n‚ùå √âchec de l'analyse: {str(e)}")
        logging.error(f"Erreur principale: {e}", exc_info=True)

if __name__ == "__main__":
    main()